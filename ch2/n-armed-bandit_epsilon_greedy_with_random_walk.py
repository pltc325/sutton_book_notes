import numpy as np
import matplotlib.pyplot as plt

'''
If we add randomness to inherent value of each action,
theoretically, constant alpha method will work better than 
sample average method because it puts more emphasis on the recent 
action than the later one.
'''

class NArmedBandit(object):
    actions = [0,1,2]

    def __init__(self,  num_actions, epsilon, num_steps, random_degree, alpha = None):
        self.num_actions = num_actions
        self.epsilon = epsilon
        self.num_steps = num_steps
        self.alpha = alpha
        self.random_degree = random_degree
        self.reset()

    def reset(self):
        self.q = np.zeros(self.num_actions)
        self.avg_reward = np.zeros((self.num_actions,))
        self.cur_step = 1
        self.rewards = np.zeros(self.num_steps)
        self.occur = np.zeros((self.num_actions,))
        self.optimal_counts = np.zeros(self.num_steps)

    def prepare(self):
        # each step, q will be updated with values generated by random walk
        ps = np.random.uniform(0,1,self.num_actions)
        self.q[ps>0.5] += 0.1 * self.random_degree
        self.q[ps<=0.5] -= 0.1 * self.random_degree
        self.max_q = np.argmax(self.q)

    def find_action(self):
        p = np.random.uniform(0, 1)
        if p > self.epsilon:
            action = np.argmax(self.avg_reward)
        else:
            action = np.random.randint(0, 10)
        self.occur[action] += 1
        return action

    def get_reward(self, action):
        reward = self.q[action] + np.random.normal(0,1)
        if self.alpha is None:
            # sample average
            self.avg_reward[action] = self.avg_reward[action] + 1/self.occur[action] * (reward - self.avg_reward[action])
        else:
            # constant alpha
            self.avg_reward[action] = self.avg_reward[action] + self.alpha * (reward - self.avg_reward[action])
        self.rewards[self.cur_step - 1] = reward
        if action == self.max_q:
            self.optimal_counts[self.cur_step - 1] += 1
        self.cur_step += 1
        return reward

    def play(self):
        self.prepare()
        self.get_reward(self.find_action())

if __name__ == '__main__':
    num_times = 200
    num_steps = 3000
    num_arms = 10
    random_degree = 1
    alphas = [None, 0.1]
    num_choices = len(alphas)
    bandits = [NArmedBandit(num_arms, 0.1, num_steps, random_degree, alpha) for alpha in alphas]
    avgs_reward_per_step = np.zeros((num_choices, num_steps))
    avgs_optimal_counts = np.zeros((num_choices, num_steps))
    for t in range(num_times):
        for band in bandits:
            band.reset()
        for step in range(num_steps):
            for band in bandits:
                band.play()

        for i in range(num_choices):
            avgs_reward_per_step[i] = avgs_reward_per_step[i] + bandits[i].rewards
            avgs_optimal_counts[i] = avgs_optimal_counts[i] + bandits[i].optimal_counts

    avgs_reward_per_step = avgs_reward_per_step / num_times
    print(np.mean(avgs_reward_per_step))
    avgs_optimal_counts = avgs_optimal_counts / num_times
    x = np.arange(num_steps)
    plt.subplot(2, 1, 1)
    for y in avgs_reward_per_step:
        plt.plot(x, y)
    plt.subplot(2,1,2)
    for y in avgs_optimal_counts:
        plt.plot(x, y)
    plt.show()